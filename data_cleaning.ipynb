{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,lit\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: demoji in /home/mhk9c/.local/lib/python3.7/site-packages (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "username = 'mhk9c'\n",
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install demoji\n",
    "sys.path.append(f'/home/{username}/.local/lib/python3.7/site-packages/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel/__main__.py:2: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "import demoji \n",
    "demoji.download_codes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/project/ds5559/team1_sp22/data/russian-troll-tweets-master\"\n",
    "reload = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "russian-troll-tweets-master\n",
      "Done loading.\n",
      "There are 2914254 tweets in this dataset\n"
     ]
    }
   ],
   "source": [
    "def load_data(_data_path, reload=True):\n",
    "    \n",
    "    dir_name = os.path.basename(os.path.normpath(_data_path))\n",
    "    print(dir_name)\n",
    "    \n",
    "    if(reload):\n",
    "        first = True\n",
    "        for file in glob.glob(f'{_data_path}/*.csv'):            \n",
    "            print(file)\n",
    "\n",
    "            if(first):\n",
    "                _df = spark.read.csv(file, header=True, inferSchema=True, mode=\"DROPMALFORMED\")                \n",
    "                _df = _df.withColumn(\"source_file\",lit(file))\n",
    "            else:\n",
    "                new_df = spark.read.csv(file, header=True, inferSchema=True, mode=\"DROPMALFORMED\")\n",
    "                new_df = new_df.withColumn(\"source_file\",lit(file))                \n",
    "                _df = _df.union(new_df)                        \n",
    "            first = False        \n",
    "            \n",
    "        _df.write.format(\"parquet\").mode(\"overwrite\").save(f\"{data_path}/{dir_name}_parquet\")\n",
    "        \n",
    "    else:\n",
    "        _df = spark.read.parquet(f\"{data_path}/{dir_name}_parquet\")\n",
    "        \n",
    "    print('Done loading.')\n",
    "    return _df\n",
    "        \n",
    "        \n",
    "df = load_data(data_path, False)\n",
    "total_tweets = df.count()\n",
    "print(f'There are {total_tweets} tweets in this dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- external_author_id: string (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- content: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- publish_date: string (nullable = true)\n",
      " |-- harvested_date: string (nullable = true)\n",
      " |-- following: string (nullable = true)\n",
      " |-- followers: string (nullable = true)\n",
      " |-- updates: string (nullable = true)\n",
      " |-- post_type: string (nullable = true)\n",
      " |-- account_type: string (nullable = true)\n",
      " |-- retweet: string (nullable = true)\n",
      " |-- account_category: string (nullable = true)\n",
      " |-- new_june_2018: string (nullable = true)\n",
      " |-- alt_external_id: string (nullable = true)\n",
      " |-- tweet_id: string (nullable = true)\n",
      " |-- article_url: string (nullable = true)\n",
      " |-- tco1_step1: string (nullable = true)\n",
      " |-- tco2_step1: string (nullable = true)\n",
      " |-- tco3_step1: string (nullable = true)\n",
      " |-- source_file: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2,096,049 english tweets in this dataset. They account for 71.924033% of the dataset.\n"
     ]
    }
   ],
   "source": [
    "sqlDF = spark.sql(\"SELECT * FROM tweets where language = 'English' \")\n",
    "english_tweets = sqlDF.count()\n",
    "print(f'There are {english_tweets:,} english tweets in this dataset. They account for {english_tweets/total_tweets:%} of the dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlDF.createOrReplaceTempView(\"english_tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlDF = spark.sql(\"SELECT content,source_file FROM english_tweets LIMIT 100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def convert_emojii(string):\n",
    "    return demoji.replace_with_desc(string, \":\")\n",
    "    \n",
    "test = convert_emojii(\"üêùüêùüêù\")   \n",
    "print(test)\n",
    "\n",
    "convert_emojii_UDF = func.udf(lambda z:convert_emojii(z),StringType())   \n",
    "\n",
    "sqlDF = sqlDF.withColumn(\"curated_contenet\", convert_emojii_UDF(col(\"content\")))\n",
    "\n",
    "sqlDF.select([\"curated_contenet\", \"source_file\"]).show(100, False)\n",
    "sqlDF.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110 Spark 3.1",
   "language": "python",
   "name": "ds5110_spark3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
